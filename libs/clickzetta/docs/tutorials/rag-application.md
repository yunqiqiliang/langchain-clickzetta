# æ„å»ºRAGåº”ç”¨å®Œæ•´æŒ‡å—

æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æ‚¨ä½¿ç”¨ LangChain ClickZetta æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åº”ç”¨ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ªä¼ä¸šçº§RAGåº”ç”¨ï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š
- æ–‡æ¡£ä¸Šä¼ å’Œå‘é‡åŒ–å­˜å‚¨
- æ™ºèƒ½æ–‡æ¡£æ£€ç´¢
- åŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”ç”Ÿæˆ
- èŠå¤©å†å²ç®¡ç†
- æ··åˆæœç´¢èƒ½åŠ›ï¼ˆå‘é‡+å…¨æ–‡ï¼‰

## ğŸ“‹ æŠ€æœ¯æ ˆ

- **æ•°æ®å­˜å‚¨**: ClickZettaï¼ˆå‘é‡å­˜å‚¨ã€å…¨æ–‡ç´¢å¼•ã€èŠå¤©å†å²ï¼‰
- **åµŒå…¥æ¨¡å‹**: çµç§¯DashScope text-embedding-v4
- **å¤§è¯­è¨€æ¨¡å‹**: é€šä¹‰åƒé—® qwen-plus
- **æ¡†æ¶**: LangChain + ClickZettaé›†æˆ

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
ç”¨æˆ·æŸ¥è¯¢ â†’ æ··åˆæ£€ç´¢ â†’ ä¸Šä¸‹æ–‡å¢å¼º â†’ LLMç”Ÿæˆ â†’ è¿”å›ç­”æ¡ˆ
    â†“           â†“            â†“          â†“
èŠå¤©å†å² â†’ å‘é‡æœç´¢+å…¨æ–‡æœç´¢ â†’ æ’åºé‡ç»„ â†’ å†å²è®°å¿†
```

## ğŸš€ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
pip install langchain-clickzetta dashscope langchain-community
```

### ç¯å¢ƒé…ç½®

```python
import os
from dotenv import load_dotenv

load_dotenv()

# ClickZettaé…ç½®
CLICKZETTA_CONFIG = {
    "service": os.getenv("CLICKZETTA_SERVICE"),
    "instance": os.getenv("CLICKZETTA_INSTANCE"),
    "workspace": os.getenv("CLICKZETTA_WORKSPACE"),
    "schema": os.getenv("CLICKZETTA_SCHEMA"),
    "username": os.getenv("CLICKZETTA_USERNAME"),
    "password": os.getenv("CLICKZETTA_PASSWORD"),
    "vcluster": os.getenv("CLICKZETTA_VCLUSTER"),
}

# çµç§¯é…ç½®
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
```

## ğŸ“ ç¬¬äºŒæ­¥ï¼šæ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–

```python
from langchain_clickzetta import (
    ClickZettaEngine,
    ClickZettaHybridStore,
    ClickZettaUnifiedRetriever,
    ClickZettaChatMessageHistory
)
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferWindowMemory

class RAGApplication:
    def __init__(self, clickzetta_config: dict, dashscope_api_key: str):
        """åˆå§‹åŒ–RAGåº”ç”¨"""

        # åˆå§‹åŒ–ClickZettaå¼•æ“
        self.engine = ClickZettaEngine(**clickzetta_config)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = DashScopeEmbeddings(
            dashscope_api_key=dashscope_api_key,
            model="text-embedding-v4"
        )

        # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self.llm = Tongyi(
            dashscope_api_key=dashscope_api_key,
            model_name="qwen-plus",
            temperature=0.1
        )

        # åˆå§‹åŒ–æ··åˆå­˜å‚¨ï¼ˆæ–‡æ¡£åº“ï¼‰
        self.document_store = ClickZettaHybridStore(
            engine=self.engine,
            embeddings=self.embeddings,
            table_name="rag_documents",
            text_analyzer="ik",  # ä¸­æ–‡åˆ†è¯
            distance_metric="cosine"
        )

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.retriever = ClickZettaUnifiedRetriever(
            hybrid_store=self.document_store,
            search_type="hybrid",
            alpha=0.5,  # å‘é‡æœç´¢å’Œå…¨æ–‡æœç´¢æƒé‡å¹³è¡¡
            k=5  # è¿”å›top-5ç»“æœ
        )

        print("âœ… RAGåº”ç”¨åˆå§‹åŒ–å®Œæˆ")

    def get_chat_history(self, session_id: str) -> ClickZettaChatMessageHistory:
        """è·å–èŠå¤©å†å²ç®¡ç†å™¨"""
        return ClickZettaChatMessageHistory(
            engine=self.engine,
            session_id=session_id,
            table_name="rag_chat_history"
        )
```

## ğŸ“š ç¬¬ä¸‰æ­¥ï¼šæ–‡æ¡£ç®¡ç†

```python
import hashlib
from typing import List
from pathlib import Path

class DocumentManager:
    def __init__(self, rag_app: RAGApplication):
        self.rag_app = rag_app

    def add_text_document(self, content: str, metadata: dict = None) -> str:
        """æ·»åŠ æ–‡æœ¬æ–‡æ¡£"""
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = hashlib.md5(content.encode()).hexdigest()

        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        document = Document(
            page_content=content,
            metadata={
                "doc_id": doc_id,
                "type": "text",
                **(metadata or {})
            }
        )

        # æ·»åŠ åˆ°æ··åˆå­˜å‚¨
        self.rag_app.document_store.add_documents([document])

        print(f"âœ… æ–‡æ¡£å·²æ·»åŠ ï¼ŒID: {doc_id}")
        return doc_id

    def add_file_document(self, file_path: str, metadata: dict = None) -> str:
        """æ·»åŠ æ–‡ä»¶æ–‡æ¡£"""
        file_path = Path(file_path)

        # è¯»å–æ–‡ä»¶å†…å®¹
        if file_path.suffix.lower() == '.txt':
            content = file_path.read_text(encoding='utf-8')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

        # æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
        file_metadata = {
            "filename": file_path.name,
            "file_path": str(file_path),
            "file_size": file_path.stat().st_size,
            **(metadata or {})
        }

        return self.add_text_document(content, file_metadata)

    def add_batch_documents(self, documents: List[dict]) -> List[str]:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        doc_ids = []

        for doc_data in documents:
            content = doc_data["content"]
            metadata = doc_data.get("metadata", {})
            doc_id = self.add_text_document(content, metadata)
            doc_ids.append(doc_id)

        print(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆï¼Œå…±{len(doc_ids)}ä¸ªæ–‡æ¡£")
        return doc_ids

# ä½¿ç”¨ç¤ºä¾‹
def load_sample_documents(doc_manager: DocumentManager):
    """åŠ è½½ç¤ºä¾‹æ–‡æ¡£"""
    sample_docs = [
        {
            "content": "äº‘å™¨ClickZettaæ˜¯æ–°ä¸€ä»£äº‘åŸç”Ÿæ¹–ä»“ä¸€ä½“åŒ–å¹³å°ï¼Œé‡‡ç”¨å¢é‡è®¡ç®—æŠ€æœ¯ï¼Œç›¸æ¯”ä¼ ç»ŸSparkæ¶æ„æ€§èƒ½æå‡10å€ã€‚æ”¯æŒå®æ—¶æ•°æ®å¤„ç†ã€æ‰¹æµä¸€ä½“ã€å­˜å‚¨è®¡ç®—åˆ†ç¦»ç­‰ç‰¹æ€§ã€‚",
            "metadata": {"category": "product", "topic": "clickzetta"}
        },
        {
            "content": "LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶åŒ…æ‹¬æ–‡æ¡£åŠ è½½å™¨ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢å™¨ã€é“¾ç­‰ã€‚æ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ•°æ®åº“ã€‚",
            "metadata": {"category": "framework", "topic": "langchain"}
        },
        {
            "content": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ã€‚é€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚",
            "metadata": {"category": "technology", "topic": "rag"}
        },
        {
            "content": "å‘é‡æ•°æ®åº“ä½¿ç”¨é«˜ç»´å‘é‡è¡¨ç¤ºæ•°æ®ï¼Œé€šè¿‡è®¡ç®—å‘é‡é—´çš„ç›¸ä¼¼åº¦æ¥å®ç°è¯­ä¹‰æœç´¢ã€‚å¸¸è§çš„è·ç¦»åº¦é‡åŒ…æ‹¬ä½™å¼¦è·ç¦»ã€æ¬§æ°è·ç¦»ç­‰ã€‚",
            "metadata": {"category": "technology", "topic": "vector"}
        }
    ]

    return doc_manager.add_batch_documents(sample_docs)
```

## ğŸ¤– ç¬¬å››æ­¥ï¼šé—®ç­”ç³»ç»Ÿ

```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import HumanMessage, AIMessage

class RAGChatBot:
    def __init__(self, rag_app: RAGApplication):
        self.rag_app = rag_app

        # åˆ›å»ºå¯¹è¯æ£€ç´¢é“¾
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.rag_app.llm,
            retriever=self.rag_app.retriever,
            return_source_documents=True,
            verbose=True
        )

    def chat(self, question: str, session_id: str) -> dict:
        """è¿›è¡Œå¯¹è¯é—®ç­”"""

        # è·å–èŠå¤©å†å²
        chat_history = self.rag_app.get_chat_history(session_id)

        # è·å–å†å²å¯¹è¯ï¼ˆæœ€è¿‘10è½®ï¼‰
        history_messages = chat_history.get_messages_by_count(10)

        # è½¬æ¢ä¸ºå¯¹è¯å†å²æ ¼å¼
        chat_history_tuples = []
        for i in range(0, len(history_messages), 2):
            if i + 1 < len(history_messages):
                human_msg = history_messages[i]
                ai_msg = history_messages[i + 1]
                if (isinstance(human_msg, HumanMessage) and
                    isinstance(ai_msg, AIMessage)):
                    chat_history_tuples.append((human_msg.content, ai_msg.content))

        # æ‰§è¡Œé—®ç­”
        result = self.qa_chain({
            "question": question,
            "chat_history": chat_history_tuples
        })

        # ä¿å­˜å½“å‰å¯¹è¯åˆ°å†å²
        chat_history.add_message(HumanMessage(content=question))
        chat_history.add_message(AIMessage(content=result["answer"]))

        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        response = {
            "question": question,
            "answer": result["answer"],
            "source_documents": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ],
            "session_id": session_id
        }

        return response

    def get_conversation_history(self, session_id: str) -> List[dict]:
        """è·å–å¯¹è¯å†å²"""
        chat_history = self.rag_app.get_chat_history(session_id)
        messages = chat_history.messages

        conversation = []
        for msg in messages:
            role = "user" if isinstance(msg, HumanMessage) else "assistant"
            conversation.append({
                "role": role,
                "content": msg.content,
                "timestamp": getattr(msg, 'timestamp', None)
            })

        return conversation
```

## ğŸ” ç¬¬äº”æ­¥ï¼šé«˜çº§æ£€ç´¢åŠŸèƒ½

```python
class AdvancedRetriever:
    def __init__(self, rag_app: RAGApplication):
        self.rag_app = rag_app

    def semantic_search(self, query: str, k: int = 5) -> List[dict]:
        """çº¯å‘é‡è¯­ä¹‰æœç´¢"""
        documents = self.rag_app.document_store.similarity_search(query, k=k)

        return [
            {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "type": "semantic"
            }
            for doc in documents
        ]

    def keyword_search(self, query: str, k: int = 5) -> List[dict]:
        """çº¯å…³é”®è¯æœç´¢"""
        # ä½¿ç”¨å…¨æ–‡æ£€ç´¢å™¨
        from langchain_clickzetta.retrievers import ClickZettaFullTextRetriever

        fulltext_retriever = ClickZettaFullTextRetriever(
            engine=self.rag_app.engine,
            table_name=self.rag_app.document_store.table_name,
            search_type="phrase",
            k=k
        )

        documents = fulltext_retriever.get_relevant_documents(query)

        return [
            {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "type": "keyword"
            }
            for doc in documents
        ]

    def hybrid_search_with_filters(
        self,
        query: str,
        filters: dict = None,
        k: int = 5
    ) -> List[dict]:
        """å¸¦è¿‡æ»¤æ¡ä»¶çš„æ··åˆæœç´¢"""

        # æ„å»ºè¿‡æ»¤æ¡ä»¶SQL
        filter_sql = ""
        if filters:
            conditions = []
            for key, value in filters.items():
                if isinstance(value, str):
                    conditions.append(f"JSON_EXTRACT(metadata, '$.{key}') = '{value}'")
                elif isinstance(value, list):
                    values_str = "', '".join(str(v) for v in value)
                    conditions.append(f"JSON_EXTRACT(metadata, '$.{key}') IN ('{values_str}')")

            if conditions:
                filter_sql = " AND " + " AND ".join(conditions)

        # æ‰§è¡Œæ··åˆæœç´¢
        retriever = ClickZettaUnifiedRetriever(
            hybrid_store=self.rag_app.document_store,
            search_type="hybrid",
            alpha=0.5,
            k=k,
            filter_sql=filter_sql
        )

        documents = retriever.invoke(query)

        return [
            {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "type": "hybrid_filtered"
            }
            for doc in documents
        ]

    def multi_strategy_search(self, query: str, k: int = 5) -> dict:
        """å¤šç­–ç•¥æœç´¢å¯¹æ¯”"""
        return {
            "semantic": self.semantic_search(query, k),
            "keyword": self.keyword_search(query, k),
            "hybrid": self.rag_app.retriever.invoke(query)
        }
```

## ğŸ“Š ç¬¬å…­æ­¥ï¼šå®Œæ•´åº”ç”¨ç¤ºä¾‹

```python
def main():
    # åˆå§‹åŒ–RAGåº”ç”¨
    rag_app = RAGApplication(CLICKZETTA_CONFIG, DASHSCOPE_API_KEY)

    # æ–‡æ¡£ç®¡ç†å™¨
    doc_manager = DocumentManager(rag_app)

    # èŠå¤©æœºå™¨äºº
    chatbot = RAGChatBot(rag_app)

    # é«˜çº§æ£€ç´¢å™¨
    advanced_retriever = AdvancedRetriever(rag_app)

    # 1. åŠ è½½ç¤ºä¾‹æ–‡æ¡£
    print("=== åŠ è½½ç¤ºä¾‹æ–‡æ¡£ ===")
    doc_ids = load_sample_documents(doc_manager)

    # 2. æµ‹è¯•ä¸åŒæ£€ç´¢ç­–ç•¥
    print("\n=== æµ‹è¯•æ£€ç´¢åŠŸèƒ½ ===")
    query = "ä»€ä¹ˆæ˜¯ClickZettaï¼Ÿ"

    # å¤šç­–ç•¥æœç´¢å¯¹æ¯”
    search_results = advanced_retriever.multi_strategy_search(query)
    print(f"æŸ¥è¯¢: {query}")

    for strategy, results in search_results.items():
        print(f"\n{strategy.upper()} æœç´¢ç»“æœ:")
        for i, result in enumerate(results[:2], 1):
            content = result.page_content if hasattr(result, 'page_content') else result['content']
            print(f"  {i}. {content[:100]}...")

    # 3. å¯¹è¯é—®ç­”æµ‹è¯•
    print("\n=== å¯¹è¯é—®ç­”æµ‹è¯• ===")
    session_id = "demo_session"

    questions = [
        "ä»€ä¹ˆæ˜¯ClickZettaï¼Ÿå®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "RAGæŠ€æœ¯æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "ClickZettaç›¸æ¯”ä¼ ç»ŸSparkæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "LangChainæ¡†æ¶åŒ…å«å“ªäº›ç»„ä»¶ï¼Ÿ"
    ]

    for question in questions:
        print(f"\nç”¨æˆ·: {question}")

        response = chatbot.chat(question, session_id)
        print(f"AI: {response['answer']}")

        # æ˜¾ç¤ºæºæ–‡æ¡£
        print("å‚è€ƒæ–‡æ¡£:")
        for i, source in enumerate(response['source_documents'][:2], 1):
            print(f"  {i}. {source['content'][:80]}...")

    # 4. æŸ¥çœ‹å¯¹è¯å†å²
    print("\n=== å¯¹è¯å†å² ===")
    history = chatbot.get_conversation_history(session_id)
    for msg in history[-4:]:  # æ˜¾ç¤ºæœ€å4æ¡æ¶ˆæ¯
        role = "ç”¨æˆ·" if msg["role"] == "user" else "AI"
        print(f"{role}: {msg['content'][:100]}...")

if __name__ == "__main__":
    main()
```

## ğŸš€ ç¬¬ä¸ƒæ­¥ï¼šWebç•Œé¢ï¼ˆå¯é€‰ï¼‰

```python
import streamlit as st

def create_streamlit_app():
    """åˆ›å»ºStreamlit Webç•Œé¢"""

    st.title("ğŸ¤– æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    st.caption("åŸºäº LangChain ClickZetta çš„RAGåº”ç”¨")

    # åˆå§‹åŒ–åº”ç”¨ï¼ˆä½¿ç”¨session stateç¼“å­˜ï¼‰
    if 'rag_app' not in st.session_state:
        with st.spinner("åˆå§‹åŒ–åº”ç”¨..."):
            st.session_state.rag_app = RAGApplication(CLICKZETTA_CONFIG, DASHSCOPE_API_KEY)
            st.session_state.chatbot = RAGChatBot(st.session_state.rag_app)

    # ä¾§è¾¹æ  - æ–‡æ¡£ç®¡ç†
    with st.sidebar:
        st.header("ğŸ“š æ–‡æ¡£ç®¡ç†")

        # æ–‡æ¡£ä¸Šä¼ 
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=['txt'])
        if uploaded_file and st.button("æ·»åŠ æ–‡æ¡£"):
            content = uploaded_file.read().decode('utf-8')
            doc_manager = DocumentManager(st.session_state.rag_app)
            doc_id = doc_manager.add_text_document(
                content,
                {"filename": uploaded_file.name}
            )
            st.success(f"æ–‡æ¡£å·²æ·»åŠ : {doc_id[:8]}...")

        # æœç´¢ç­–ç•¥é€‰æ‹©
        st.header("ğŸ” æœç´¢è®¾ç½®")
        search_strategy = st.selectbox(
            "æ£€ç´¢ç­–ç•¥",
            ["hybrid", "semantic", "keyword"]
        )

    # ä¸»ç•Œé¢ - å¯¹è¯
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    # ä¼šè¯ID
    session_id = st.text_input("ä¼šè¯ID", value="default_session")

    # èŠå¤©å†å²æ˜¾ç¤º
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
            if "sources" in message:
                with st.expander("å‚è€ƒæ–‡æ¡£"):
                    for i, source in enumerate(message["sources"], 1):
                        st.text(f"{i}. {source['content'][:200]}...")

    # ç”¨æˆ·è¾“å…¥
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.write(question)

        # ç”Ÿæˆå›ç­”
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                response = st.session_state.chatbot.chat(question, session_id)

                # æ˜¾ç¤ºå›ç­”
                st.write(response["answer"])

                # æ˜¾ç¤ºæºæ–‡æ¡£
                with st.expander("å‚è€ƒæ–‡æ¡£"):
                    for i, source in enumerate(response["source_documents"], 1):
                        st.text(f"{i}. {source['content'][:200]}...")

                # ä¿å­˜åˆ°ä¼šè¯
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response["answer"],
                    "sources": response["source_documents"]
                })

# è¿è¡ŒStreamlitåº”ç”¨
# streamlit run rag_app.py
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®å­˜å‚¨ä¼˜åŒ–

```python
# ä½¿ç”¨åˆ†åŒºè¡¨æé«˜æŸ¥è¯¢æ€§èƒ½
create_partitioned_table_sql = """
CREATE TABLE rag_documents_partitioned (
    id String,
    content String,
    embedding Array(Float32),
    metadata String,
    created_at Timestamp DEFAULT CURRENT_TIMESTAMP
)
PARTITION BY toYYYYMM(created_at)
"""

# å»ºç«‹é€‚å½“çš„ç´¢å¼•
create_indexes_sql = [
    "CREATE INDEX idx_metadata ON rag_documents (metadata)",
    "CREATE INVERTED INDEX idx_content ON rag_documents (content) WITH ANALYZER='ik'",
    "CREATE VECTOR INDEX idx_embedding ON rag_documents (embedding)"
]
```

### 2. æ£€ç´¢ä¼˜åŒ–

```python
# ç¼“å­˜é¢‘ç¹æŸ¥è¯¢çš„ç»“æœ
from functools import lru_cache

class CachedRetriever:
    def __init__(self, retriever):
        self.retriever = retriever

    @lru_cache(maxsize=100)
    def cached_search(self, query: str, k: int = 5):
        return self.retriever.invoke(query)
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡æ·»åŠ æ–‡æ¡£
def batch_add_documents(document_store, documents, batch_size=100):
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        document_store.add_documents(batch)
        print(f"å·²å¤„ç† {min(i + batch_size, len(documents))}/{len(documents)} æ–‡æ¡£")
```

## ğŸ¯ æ€»ç»“

æœ¬æ•™ç¨‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangChain ClickZetta æ„å»ºä¸€ä¸ªå®Œæ•´çš„RAGåº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š

âœ… **æ ¸å¿ƒåŠŸèƒ½å®ç°**
- æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨
- æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…¨æ–‡ï¼‰
- å¯¹è¯é—®ç­”ç”Ÿæˆ
- èŠå¤©å†å²ç®¡ç†

âœ… **é«˜çº§ç‰¹æ€§**
- å¤šç­–ç•¥æ£€ç´¢å¯¹æ¯”
- è¿‡æ»¤æ¡ä»¶æœç´¢
- æ‰¹é‡æ–‡æ¡£å¤„ç†
- Webç•Œé¢é›†æˆ

âœ… **ç”Ÿäº§å°±ç»ª**
- æ€§èƒ½ä¼˜åŒ–å»ºè®®
- é”™è¯¯å¤„ç†æœºåˆ¶
- å¯æ‰©å±•æ¶æ„è®¾è®¡
- å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹

é€šè¿‡è¿™ä¸ªRAGåº”ç”¨ï¼Œæ‚¨å¯ä»¥æ„å»ºæ™ºèƒ½å®¢æœã€çŸ¥è¯†é—®ç­”ã€æ–‡æ¡£åŠ©æ‰‹ç­‰å¤šç§AIåº”ç”¨ã€‚ClickZettaçš„é«˜æ€§èƒ½å’ŒLangChainçš„ä¸°å¯Œç”Ÿæ€ä¸ºæ‚¨æä¾›äº†å¼ºå¤§çš„æŠ€æœ¯åŸºç¡€ã€‚